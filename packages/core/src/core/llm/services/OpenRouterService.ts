import OpenAI from 'openai';
import {
  ILLMService,
  LLMResponse,
  LLMChatOptions,
  ImageData,
  ToolSet,
  UnifiedToolManager,
} from '../types/index.js';
import { logger } from '../../../utils/logger.js';
import { sleep, normalizeToolCalls } from '../utils/helpers.js';

/**
 * OpenRouter LLM æœåŠ¡
 * ä½¿ç”¨ OpenAI SDKï¼ˆOpenRouter å…¼å®¹ OpenAI APIï¼‰
 *
 * æä¾›ä¸¤ç§ä½¿ç”¨æ–¹å¼ï¼š
 * 1. complete() - ä¸Šä¸‹æ–‡è¡¥å…¨ï¼ˆæ¨èï¼‰
 * 2. generate() - å†…ç½®å·¥å…·è°ƒç”¨å¾ªç¯ï¼ˆå¯é€‰ï¼‰
 */
export class OpenRouterService implements ILLMService {
  private client: OpenAI;
  private model: string;
  private maxRetries: number;
  private toolManager?: UnifiedToolManager;
  private maxIterations: number;

  constructor(
    openai: OpenAI,
    model: string,
    options?: {
      baseURL?: string;
      maxRetries?: number;
      toolManager?: UnifiedToolManager;
      maxIterations?: number;
    }
  ) {
    this.client = openai;
    this.model = model;
    this.maxRetries = options?.maxRetries || 3;
    this.toolManager = options?.toolManager;
    this.maxIterations = options?.maxIterations || 5;

    logger.debug(`åˆå§‹åŒ– OpenRouterService: model=${model}`);
  }

  /**
   * ä¸º Claude æ¨¡å‹æ·»åŠ æ™ºèƒ½ç¼“å­˜æ§åˆ¶
   * - åªç¼“å­˜è¶…è¿‡é˜ˆå€¼çš„å¤§æ–‡æœ¬ï¼ˆ1000+ å­—ç¬¦ï¼‰
   * - æ”¯æŒ system å’Œ user æ¶ˆæ¯
   * - æœ€å¤šä½¿ç”¨ 4 ä¸ªç¼“å­˜æ–­ç‚¹ï¼ˆClaude é™åˆ¶ï¼‰
   * - TTL è®¾ç½®ä¸º 1hï¼Œé€‚åˆ 15-20 åˆ†é’Ÿçš„å¯¹è¯åœºæ™¯
   */
  private addCacheControlForClaude(messages: any[]): any[] {
    if (!this.model.includes('claude') || !this.model.includes('gemini')) {
      return messages;
    }

    const CACHE_THRESHOLD = 1000; // åªç¼“å­˜è¶…è¿‡ 1000 å­—ç¬¦çš„å†…å®¹
    const MAX_CACHE_BREAKPOINTS = 4;
    let cacheBreakpointsUsed = 0;

    return messages.map((message) => {
      // åªå¤„ç† system å’Œ user æ¶ˆæ¯
      if (message.role !== 'system' && message.role !== 'user') {
        return message;
      }

      // å·²è¾¾åˆ°ç¼“å­˜æ–­ç‚¹ä¸Šé™
      if (cacheBreakpointsUsed >= MAX_CACHE_BREAKPOINTS) {
        return message;
      }

      const content = message.content;

      // å¤„ç†å­—ç¬¦ä¸²å†…å®¹
      if (typeof content === 'string') {
        if (content.length >= CACHE_THRESHOLD) {
          cacheBreakpointsUsed++;
          return {
            ...message,
            content: [
              {
                type: 'text',
                text: content,
                cache_control: { type: 'ephemeral', ttl: '1h' },
              },
            ],
          };
        }
        return message;
      }

      // å¤„ç†æ•°ç»„å†…å®¹ - ä¸ºå¤§æ–‡æœ¬å—æ·»åŠ ç¼“å­˜
      if (Array.isArray(content)) {
        const processedContent = content.map((item) => {
          if (
            item.type === 'text' &&
            typeof item.text === 'string' &&
            item.text.length >= CACHE_THRESHOLD &&
            !item.cache_control &&
            cacheBreakpointsUsed < MAX_CACHE_BREAKPOINTS
          ) {
            cacheBreakpointsUsed++;
            return {
              ...item,
              cache_control: { type: 'ephemeral', ttl: '1h' },
            };
          }
          return item;
        });
        return { ...message, content: processedContent };
      }

      return message;
    });
  }

  /*
   * æ¥æ”¶æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡ï¼ˆæ¶ˆæ¯å†å²ï¼‰ï¼Œè¿”å›æ¨¡å‹å“åº”
   * @param messages - ä¸Šä¸‹æ–‡æ¶ˆæ¯åˆ—è¡¨
   * @param tools - å¯ç”¨çš„å·¥å…·å®šä¹‰åˆ—è¡¨
   * @param options - ç”Ÿæˆå‚æ•°ï¼ˆtemperature, maxTokens ç­‰ï¼‰
   * @returns æ¨¡å‹å“åº”ï¼ˆåŒ…å«å†…å®¹ã€å·¥å…·è°ƒç”¨ã€ä½¿ç”¨ç»Ÿè®¡ï¼‰
   */
  async complete(messages: any[], tools?: any[], options?: LLMChatOptions): Promise<LLMResponse> {
    let attempt = 0;

    while (attempt < this.maxRetries) {
      attempt++;

      try {
        logger.debug(
          `è°ƒç”¨ OpenRouter API (å°è¯• ${attempt}/${this.maxRetries}): ${messages.length} æ¡æ¶ˆæ¯, ${tools?.length || 0} ä¸ªå·¥å…·`
        );

        const response = await this.client.chat.completions.create({
          model: this.model,
          messages: this.addCacheControlForClaude(messages),
          tools: tools && tools.length > 0 ? tools : undefined,
          tool_choice: options?.toolChoice,
          temperature: options?.temperature,
          max_tokens: options?.maxTokens,
          top_p: options?.topP,
          frequency_penalty: options?.frequencyPenalty,
          presence_penalty: options?.presencePenalty,
          stop: options?.stop,
          response_format: options?.responseFormat,
        });
        console.dir(response, { depth: null });

        const message = response.choices[0]?.message;
        if (!message) {
          throw new Error('OpenRouter API è¿”å›ç©ºå“åº”');
        }

        const result: LLMResponse = {
          content: message.content || '',
          toolCalls: normalizeToolCalls(message.tool_calls),
          finishReason: response.choices[0]?.finish_reason,
          usage: response.usage
            ? {
                promptTokens: response.usage.prompt_tokens,
                completionTokens: response.usage.completion_tokens,
                totalTokens: response.usage.total_tokens,
                reasoningTokens: response.usage.completion_tokens_details?.reasoning_tokens,
              }
            : undefined,
        };

        logger.debug(
          `OpenRouter å“åº”: ${result.content.slice(0, 100)}${result.content.length > 100 ? '...' : ''}, å·¥å…·è°ƒç”¨: ${result.toolCalls?.length || 0}`
        );

        return result;
      } catch (error: any) {
        logger.error(`OpenRouter API è°ƒç”¨å¤±è´¥ (${attempt}/${this.maxRetries}): ${error.message}`);
        if (attempt >= this.maxRetries) {
          throw new Error(`OpenRouter API è°ƒç”¨å¤±è´¥: ${error.message}`);
        }

        // æŒ‡æ•°é€€é¿
        const delay = 500 * attempt;
        logger.debug(`ç­‰å¾… ${delay}ms åé‡è¯•...`);
        await sleep(delay);
      }
    }

    throw new Error('Unreachable');
  }

  /**
   * ç®€å•å¯¹è¯ï¼šæ— å·¥å…·ï¼Œå•æ¬¡è°ƒç”¨
   * @param userInput - ç”¨æˆ·è¾“å…¥
   * @param systemPrompt - å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
   */
  async simpleChat(userInput: string, systemPrompt?: string): Promise<string> {
    const messages: any[] = [];

    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }

    messages.push({ role: 'user', content: userInput });

    const response = await this.complete(messages);
    return response.content;
  }

  /**
   * è·å–é…ç½®ä¿¡æ¯
   */
  getConfig(): { provider: string; model: string } {
    return {
      provider: 'openrouter',
      model: this.model,
    };
  }

  /**
   * å®Œæ•´æ–¹æ³•ï¼šæ”¯æŒå·¥å…·è°ƒç”¨å¾ªç¯
   * éœ€è¦åœ¨æ„é€ æ—¶ä¼ å…¥ toolManager
   * @param userInput - ç”¨æˆ·è¾“å…¥
   * @param imageData - å¯é€‰çš„å›¾ç‰‡æ•°æ®
   * @param _stream - æ˜¯å¦æµå¼è¾“å‡ºï¼ˆæš‚æœªå®ç°ï¼‰
   */
  async generate(userInput: string, imageData?: ImageData, _stream?: boolean): Promise<string> {
    if (!this.toolManager) {
      throw new Error('generate() æ–¹æ³•éœ€è¦ toolManagerï¼Œè¯·åœ¨æ„é€ æ—¶ä¼ å…¥æˆ–ä½¿ç”¨ chat() æ–¹æ³•');
    }

    // åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨
    const messages: any[] = [
      {
        role: 'system',
        content: 'ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚',
      },
    ];

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    const userMessage: any = { role: 'user', content: userInput };
    if (imageData) {
      userMessage.content = [
        { type: 'text', text: userInput },
        imageData.url
          ? { type: 'image_url', image_url: { url: imageData.url } }
          : {
              type: 'image_url',
              image_url: {
                url: `data:${imageData.mimeType || 'image/png'};base64,${imageData.base64}`,
              },
            },
      ];
    }
    messages.push(userMessage);

    // è·å–å¯ç”¨å·¥å…·
    const availableTools = await this.toolManager.getToolsForProvider('openrouter');

    // å·¥å…·è°ƒç”¨å¾ªç¯
    let iteration = 0;
    while (iteration < this.maxIterations) {
      iteration++;
      logger.debug(`å·¥å…·è°ƒç”¨è¿­ä»£: ${iteration}/${this.maxIterations}`);

      // è°ƒç”¨ LLM
      const response = await this.complete(messages, availableTools, {
        toolChoice: iteration === 1 ? 'auto' : undefined,
      });

      // æ— å·¥å…·è°ƒç”¨ï¼Œè¿”å›ç»“æœ
      if (!response.toolCalls || response.toolCalls.length === 0) {
        logger.debug('æ— å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆç»“æœ');
        return response.content;
      }

      // è®°å½•æ€è€ƒå†…å®¹
      if (response.content) {
        logger.info(`ğŸ’­ åŠ©æ‰‹æ€è€ƒ: ${response.content}`);
      }

      // æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ï¼‰
      messages.push({
        role: 'assistant',
        content: response.content || null,
        tool_calls: response.toolCalls,
      });

      // æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
      for (const toolCall of response.toolCalls) {
        if (toolCall.type !== 'function' || !toolCall.function) {
          continue;
        }

        logger.info(`ğŸ”§ ä½¿ç”¨å·¥å…·: ${toolCall.function.name}`);

        try {
          const args = JSON.parse(toolCall.function.arguments);
          const result = await this.toolManager.executeTool(toolCall.function.name, args);

          logger.info(`âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: ${JSON.stringify(result).slice(0, 200)}`);

          // æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœ
          messages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify(result),
          });
        } catch (error: any) {
          logger.error(`âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: ${error.message}`);

          // æ·»åŠ é”™è¯¯ç»“æœ
          messages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify({ error: error.message }),
          });
        }
      }
    }

    throw new Error(`è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° (${this.maxIterations})`);
  }

  /**
   * è·å–æ‰€æœ‰å¯ç”¨å·¥å…·
   */
  async getAllTools(): Promise<ToolSet> {
    if (!this.toolManager) {
      return {};
    }
    return this.toolManager.getAllTools();
  }

  /**
   * æµå¼å¯¹è¯ï¼ˆé¢„ç•™æ¥å£ï¼‰
   * TODO: å®ç°æµå¼å“åº”
   */
  async chatStream(
    messages: any[],
    tools?: any[],
    options?: LLMChatOptions
  ): Promise<AsyncIterable<LLMResponse>> {
    throw new Error('æµå¼å“åº”æš‚æœªå®ç°');
  }
}
